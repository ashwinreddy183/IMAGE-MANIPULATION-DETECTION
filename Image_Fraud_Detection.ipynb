{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIp0GrBOsf-G",
        "outputId": "9ab3ed72-2f88-4e46-e714-c1a85ab7c9f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sobel(in_chan, out_chan):\n",
        "    filter_x = np.array([\n",
        "        [1, 0, -1],\n",
        "        [2, 0, -2],\n",
        "        [1, 0, -1],\n",
        "    ]).astype(np.float32)\n",
        "    filter_y = np.array([\n",
        "        [1, 2, 1],\n",
        "        [0, 0, 0],\n",
        "        [-1, -2, -1],\n",
        "    ]).astype(np.float32)\n",
        "\n",
        "    filter_x = filter_x.reshape((1, 1, 3, 3))\n",
        "    filter_x = np.repeat(filter_x, in_chan, axis=1)\n",
        "    filter_x = np.repeat(filter_x, out_chan, axis=0)\n",
        "\n",
        "    filter_y = filter_y.reshape((1, 1, 3, 3))\n",
        "    filter_y = np.repeat(filter_y, in_chan, axis=1)\n",
        "    filter_y = np.repeat(filter_y, out_chan, axis=0)\n",
        "\n",
        "    filter_x = torch.from_numpy(filter_x)\n",
        "    filter_y = torch.from_numpy(filter_y)\n",
        "    filter_x = nn.Parameter(filter_x, requires_grad=False)\n",
        "    filter_y = nn.Parameter(filter_y, requires_grad=False)\n",
        "    conv_x = nn.Conv2d(in_chan, out_chan, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    conv_x.weight = filter_x\n",
        "    conv_y = nn.Conv2d(in_chan, out_chan, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    conv_y.weight = filter_y\n",
        "    sobel_x = nn.Sequential(conv_x, nn.BatchNorm2d(out_chan))\n",
        "    sobel_y = nn.Sequential(conv_y, nn.BatchNorm2d(out_chan))\n",
        "    return sobel_x, sobel_y\n",
        "\n",
        "def run_sobel(conv_x, conv_y, input):\n",
        "    g_x = conv_x(input)\n",
        "    g_y = conv_y(input)\n",
        "    g = torch.sqrt(torch.pow(g_x, 2) + torch.pow(g_y, 2))\n",
        "    return torch.sigmoid(g) * input"
      ],
      "metadata": {
        "id": "3ZcJ1R1MAd4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip3 install Cython\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import skimage.transform as skt\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def get_sobel(in_chan, out_chan):\n",
        "    filter_x = np.array([\n",
        "        [1, 0, -1],\n",
        "        [2, 0, -2],\n",
        "        [1, 0, -1],\n",
        "    ]).astype(np.float32)\n",
        "    filter_y = np.array([\n",
        "        [1, 2, 1],\n",
        "        [0, 0, 0],\n",
        "        [-1, -2, -1],\n",
        "    ]).astype(np.float32)\n",
        "\n",
        "    filter_x = filter_x.reshape((1, 1, 3, 3))\n",
        "    filter_x = np.repeat(filter_x, in_chan, axis=1)\n",
        "    filter_x = np.repeat(filter_x, out_chan, axis=0)\n",
        "\n",
        "    filter_y = filter_y.reshape((1, 1, 3, 3))\n",
        "    filter_y = np.repeat(filter_y, in_chan, axis=1)\n",
        "    filter_y = np.repeat(filter_y, out_chan, axis=0)\n",
        "\n",
        "    filter_x = torch.from_numpy(filter_x)\n",
        "    filter_y = torch.from_numpy(filter_y)\n",
        "    filter_x = nn.Parameter(filter_x, requires_grad=False)\n",
        "    filter_y = nn.Parameter(filter_y, requires_grad=False)\n",
        "    conv_x = nn.Conv2d(in_chan, out_chan, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    conv_x.weight = filter_x\n",
        "    conv_y = nn.Conv2d(in_chan, out_chan, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    conv_y.weight = filter_y\n",
        "    sobel_x = nn.Sequential(conv_x, nn.BatchNorm2d(out_chan))\n",
        "    sobel_y = nn.Sequential(conv_y, nn.BatchNorm2d(out_chan))\n",
        "    return sobel_x, sobel_y\n",
        "\n",
        "def run_sobel(conv_x, conv_y, input):\n",
        "    g_x = conv_x(input)\n",
        "    g_y = conv_y(input)\n",
        "    g = torch.sqrt(torch.pow(g_x, 2) + torch.pow(g_y, 2))\n",
        "    return torch.sigmoid(g) * input\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BayarConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1, padding=0):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.minus1 = (torch.ones(self.in_channels, self.out_channels, 1) * -1.000)\n",
        "\n",
        "        super(BayarConv2d, self).__init__()\n",
        "        # only (kernel_size ** 2 - 1) trainable params as the center element is always -1\n",
        "        self.kernel = nn.Parameter(torch.rand(self.in_channels, self.out_channels, kernel_size ** 2 - 1),\n",
        "                                   requires_grad=True)\n",
        "\n",
        "\n",
        "    def bayarConstraint(self):\n",
        "        self.kernel.data = self.kernel.permute(2, 0, 1)\n",
        "        self.kernel.data = torch.div(self.kernel.data, self.kernel.data.sum(0))\n",
        "        self.kernel.data = self.kernel.permute(1, 2, 0)\n",
        "        ctr = self.kernel_size ** 2 // 2\n",
        "        real_kernel = torch.cat((self.kernel[:, :, :ctr], self.minus1.to(self.kernel.device), self.kernel[:, :, ctr:]), dim=2)\n",
        "        real_kernel = real_kernel.reshape((self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))\n",
        "        return real_kernel\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.conv2d(x, self.bayarConstraint(), stride=self.stride, padding=self.padding)\n",
        "        return x\n",
        "\n",
        "def rgb2gray(rgb):\n",
        "    b, g, r = rgb[:, 0, :, :], rgb[:, 1, :, :], rgb[:, 2, :, :]\n",
        "    gray = 0.2989*r + 0.5870*g + 0.1140*b\n",
        "    gray = torch.unsqueeze(gray, 1)\n",
        "    return gray\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "}\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, rate=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=rate, dilation=rate, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Define the ResNet model\n",
        "class ResNet(nn.Module):\n",
        "     def __init__(self, block, layers, num_classes=1000, n_input=3):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(n_input, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        rates = [1, 2, 4]\n",
        "        self.layer4 = self._make_deeplabv3_layer(block, 512, layers[3], rates=rates, stride=1)  # stride 2 => stride 1\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "     def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "     def _make_deeplabv3_layer(self, block, planes, blocks, rates, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, rate=rates[i]))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "     def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def resnet(pretrained=False, layers=[3,4,6,3], backbone='resnet50', n_input=3, **kwargs):\n",
        "     \"\"\"Constructs a ResNet-50 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "     model = ResNet(Bottleneck, layers, n_input=n_input, **kwargs)\n",
        "\n",
        "     pretrain_dict = model_zoo.load_url(model_urls[backbone])\n",
        "     try:\n",
        "         model.load_state_dict(pretrain_dict,strict=False)\n",
        "     except:\n",
        "         print(\"loss conv1\")\n",
        "         model_dict = {}\n",
        "         for k, v in pretrain_dict.items():\n",
        "             if k in pretrain_dict and 'conv1' not in k:\n",
        "                 model_dict[k] = v\n",
        "         model.load_state_dict(model_dict, strict=False)\n",
        "     print(\"load pretrain success\")\n",
        "     return model\n",
        "\n",
        "class ResNet50(nn.Module):\n",
        "    def __init__(self, pretrained=True,n_input=3):\n",
        "        \"\"\"Declare all needed layers.\"\"\"\n",
        "        super(ResNet50, self).__init__()\n",
        "        self.model = resnet(n_input=n_input, pretrained=pretrained, layers=[3, 4, 6, 3], backbone='resnet50')\n",
        "        self.relu = self.model.relu  # Place a hook\n",
        "\n",
        "        layers_cfg = [4, 5, 6, 7]\n",
        "        self.blocks = []\n",
        "        for i, num_this_layer in enumerate(layers_cfg):\n",
        "            self.blocks.append(list(self.model.children())[num_this_layer])\n",
        "\n",
        "    def base_forward(self, x):\n",
        "        feature_map = []\n",
        "        x = self.model.conv1(x)\n",
        "        x = self.model.bn1(x)\n",
        "        x = self.model.relu(x)\n",
        "        x = self.model.maxpool(x)\n",
        "\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            feature_map.append(x)\n",
        "\n",
        "        out = nn.AvgPool2d(x.shape[2:])(x).view(x.shape[0], -1)\n",
        "\n",
        "        return feature_map, out\n",
        "\n",
        "\n",
        "class ERB(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ERB, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x, relu=True):\n",
        "        x = self.conv1(x)\n",
        "        res = self.conv2(x)\n",
        "        res = self.bn(res)\n",
        "        res = self.relu(res)\n",
        "        res = self.conv3(res)\n",
        "        if relu:\n",
        "            return self.relu(x + res)\n",
        "        else:\n",
        "            return x + res\n",
        "\n",
        "class MVSSNet(ResNet50):\n",
        "    def __init__(self, nclass, aux=False, sobel=False, constrain=False, n_input=3, **kwargs):\n",
        "        super(MVSSNet, self).__init__(pretrained=True, n_input=n_input)\n",
        "        self.num_class = nclass\n",
        "        self.aux = aux\n",
        "\n",
        "        self.__setattr__('exclusive', ['head'])\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
        "        self.upsample_4 = nn.Upsample(scale_factor=4, mode=\"bilinear\", align_corners=True)\n",
        "        self.sobel = sobel\n",
        "        self.constrain = constrain\n",
        "\n",
        "        self.erb_db_1 = ERB(256, self.num_class)\n",
        "        self.erb_db_2 = ERB(512, self.num_class)\n",
        "        self.erb_db_3 = ERB(1024, self.num_class)\n",
        "        self.erb_db_4 = ERB(2048, self.num_class)\n",
        "\n",
        "        self.erb_trans_1 = ERB(self.num_class, self.num_class)\n",
        "        self.erb_trans_2 = ERB(self.num_class, self.num_class)\n",
        "        self.erb_trans_3 = ERB(self.num_class, self.num_class)\n",
        "\n",
        "        if self.sobel:\n",
        "            print(\"----------use sobel-------------\")\n",
        "            self.sobel_x1, self.sobel_y1 = get_sobel(256, 1)\n",
        "            self.sobel_x2, self.sobel_y2 = get_sobel(512, 1)\n",
        "            self.sobel_x3, self.sobel_y3 = get_sobel(1024, 1)\n",
        "            self.sobel_x4, self.sobel_y4 = get_sobel(2048, 1)\n",
        "\n",
        "        if self.constrain:\n",
        "            print(\"----------use constrain-------------\")\n",
        "            self.noise_extractor = ResNet50(n_input=3, pretrained=True)\n",
        "            self.constrain_conv = BayarConv2d(in_channels=1, out_channels=3, padding=2)\n",
        "            self.head = _DAHead(2048+2048, self.num_class, aux, **kwargs)\n",
        "        else:\n",
        "            self.head = _DAHead(2048, self.num_class, aux, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print('actual shape check:',x.shape)\n",
        "        print('device of x:',x.device)\n",
        "        size = x.size()[2:]\n",
        "        input_ = x.clone()\n",
        "        feature_map, _ = self.base_forward(input_)\n",
        "        c1, c2, c3, c4 = feature_map\n",
        "\n",
        "        if self.sobel:\n",
        "            res1 = self.erb_db_1(run_sobel(self.sobel_x1, self.sobel_y1, c1))\n",
        "            res1 = self.erb_trans_1(res1 + self.upsample(self.erb_db_2(run_sobel(self.sobel_x2, self.sobel_y2, c2))))\n",
        "            res1 = self.erb_trans_2(res1 + self.upsample_4(self.erb_db_3(run_sobel(self.sobel_x3, self.sobel_y3, c3))))\n",
        "            res1 = self.erb_trans_3(res1 + self.upsample_4(self.erb_db_4(run_sobel(self.sobel_x4, self.sobel_y4, c4))), relu=False)\n",
        "\n",
        "        else:\n",
        "            res1 = self.erb_db_1(c1)\n",
        "            res1 = self.erb_trans_1(res1 + self.upsample(self.erb_db_2(c2)))\n",
        "            res1 = self.erb_trans_2(res1 + self.upsample_4(self.erb_db_3(c3)))\n",
        "            res1 = self.erb_trans_3(res1 + self.upsample_4(self.erb_db_4(c4)), relu=False)\n",
        "\n",
        "        if self.constrain:\n",
        "            x = rgb2gray(x)\n",
        "            x = self.constrain_conv(x)\n",
        "            constrain_features, _ = self.noise_extractor.base_forward(x)\n",
        "            constrain_feature = constrain_features[-1]\n",
        "            c4 = torch.cat([c4, constrain_feature], dim=1)\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        x = self.head(c4)\n",
        "        x0 = F.interpolate(x[0], size, mode='bilinear', align_corners=True)\n",
        "        outputs.append(x0)\n",
        "\n",
        "        if self.aux:\n",
        "            x1 = F.interpolate(x[1], size, mode='bilinear', align_corners=True)\n",
        "            x2 = F.interpolate(x[2], size, mode='bilinear', align_corners=True)\n",
        "            outputs.append(x1)\n",
        "            outputs.append(x2)\n",
        "\n",
        "        print(res1.shape)\n",
        "        print(x0.shape)\n",
        "\n",
        "        return res1, x0\n",
        "\n",
        "\n",
        "class _PositionAttentionModule(nn.Module):\n",
        "    \"\"\" Position attention module\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, **kwargs):\n",
        "        super(_PositionAttentionModule, self).__init__()\n",
        "        self.conv_b = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.conv_c = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
        "        self.conv_d = nn.Conv2d(in_channels, in_channels, 1)\n",
        "        self.alpha = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, _, height, width = x.size()\n",
        "        feat_b = self.conv_b(x).view(batch_size, -1, height * width).permute(0, 2, 1)\n",
        "        feat_c = self.conv_c(x).view(batch_size, -1, height * width)\n",
        "        attention_s = self.softmax(torch.bmm(feat_b, feat_c))\n",
        "        feat_d = self.conv_d(x).view(batch_size, -1, height * width)\n",
        "        feat_e = torch.bmm(feat_d, attention_s.permute(0, 2, 1)).view(batch_size, -1, height, width)\n",
        "        out = self.alpha * feat_e + x\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class _ChannelAttentionModule(nn.Module):\n",
        "    \"\"\"Channel attention module\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(_ChannelAttentionModule, self).__init__()\n",
        "        self.beta = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, _, height, width = x.size()\n",
        "        feat_a = x.view(batch_size, -1, height * width)\n",
        "        feat_a_transpose = x.view(batch_size, -1, height * width).permute(0, 2, 1)\n",
        "        attention = torch.bmm(feat_a, feat_a_transpose)\n",
        "        attention_new = torch.max(attention, dim=-1, keepdim=True)[0].expand_as(attention) - attention\n",
        "        attention = self.softmax(attention_new)\n",
        "\n",
        "        feat_e = torch.bmm(attention, feat_a).view(batch_size, -1, height, width)\n",
        "        out = self.beta * feat_e + x\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class _DAHead(nn.Module):\n",
        "    def __init__(self, in_channels, nclass, aux=True, norm_layer=nn.BatchNorm2d, norm_kwargs=None, **kwargs):\n",
        "        super(_DAHead, self).__init__()\n",
        "        self.aux = aux\n",
        "        inter_channels = in_channels // 4\n",
        "        self.conv_p1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
        "            norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs)),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.conv_c1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\n",
        "            norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs)),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.pam = _PositionAttentionModule(inter_channels, **kwargs)\n",
        "        self.cam = _ChannelAttentionModule(**kwargs)\n",
        "        self.conv_p2 = nn.Sequential(\n",
        "            nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n",
        "            norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs)),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.conv_c2 = nn.Sequential(\n",
        "            nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\n",
        "            norm_layer(inter_channels, **({} if norm_kwargs is None else norm_kwargs)),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Conv2d(inter_channels, nclass, 1)\n",
        "        )\n",
        "        if aux:\n",
        "            self.conv_p3 = nn.Sequential(\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Conv2d(inter_channels, nclass, 1)\n",
        "            )\n",
        "            self.conv_c3 = nn.Sequential(\n",
        "                nn.Dropout(0.1),\n",
        "                nn.Conv2d(inter_channels, nclass, 1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat_p = self.conv_p1(x)\n",
        "        feat_p = self.pam(feat_p)\n",
        "        feat_p = self.conv_p2(feat_p)\n",
        "\n",
        "        feat_c = self.conv_c1(x)\n",
        "        feat_c = self.cam(feat_c)\n",
        "        feat_c = self.conv_c2(feat_c)\n",
        "\n",
        "        feat_fusion = feat_p + feat_c\n",
        "\n",
        "        outputs = []\n",
        "        fusion_out = self.out(feat_fusion)\n",
        "        outputs.append(fusion_out)\n",
        "        if self.aux:\n",
        "            p_out = self.conv_p3(feat_p)\n",
        "            c_out = self.conv_c3(feat_c)\n",
        "            outputs.append(p_out)\n",
        "            outputs.append(c_out)\n",
        "\n",
        "        return tuple(outputs)\n",
        "\n",
        "\n",
        "def get_mvss(backbone='resnet50', pretrained_base=True, nclass=1, sobel=True, n_input=3, constrain=True, **kwargs):\n",
        "    model = MVSSNet(nclass, backbone=backbone,\n",
        "                    pretrained_base=pretrained_base,\n",
        "                    sobel=sobel,\n",
        "                    n_input=n_input,\n",
        "                    constrain=constrain,\n",
        "                    **kwargs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "MxM8m14CtO_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8158a14f-7884-4406-97c1-32b2984629e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (3.0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn([1,3,256,256])"
      ],
      "metadata": {
        "id": "vLpJcMtP5R8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_mvss()\n",
        "out = model(x)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbnEPvh55jTx",
        "outputId": "14d80828-e007-4eb4-fcd4-2914c95edf84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 176MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load pretrain success\n",
            "----------use sobel-------------\n",
            "----------use constrain-------------\n",
            "load pretrain success\n",
            "actual shape check: torch.Size([1, 3, 256, 256])\n",
            "device of x: cpu\n",
            "torch.Size([1, 1, 64, 64])\n",
            "torch.Size([1, 1, 256, 256])\n",
            "(tensor([[[[ 1.1001,  1.2593,  1.2432,  ...,  1.1124,  0.8553,  0.5751],\n",
            "          [ 0.9826,  0.6777,  0.4448,  ...,  0.5605,  0.3095,  0.5068],\n",
            "          [ 0.8470,  0.6108,  0.2798,  ...,  0.4275,  0.2531,  0.4747],\n",
            "          ...,\n",
            "          [ 1.5506,  0.5496,  0.4395,  ...,  0.8315, -0.0549,  0.8615],\n",
            "          [ 2.1656,  2.2038,  0.9866,  ...,  0.8629, -0.3035,  0.3360],\n",
            "          [-1.3258,  1.2109,  0.2265,  ...,  0.1600,  0.1871,  0.2449]]]],\n",
            "       grad_fn=<AddBackward0>), tensor([[[[-0.5448, -0.4801, -0.4154,  ...,  0.2655,  0.2489,  0.2323],\n",
            "          [-0.5611, -0.4992, -0.4374,  ...,  0.2943,  0.2813,  0.2683],\n",
            "          [-0.5773, -0.5184, -0.4594,  ...,  0.3232,  0.3137,  0.3043],\n",
            "          ...,\n",
            "          [ 0.1973,  0.1892,  0.1812,  ..., -0.0667, -0.0655, -0.0642],\n",
            "          [ 0.2017,  0.1926,  0.1835,  ..., -0.1188, -0.1203, -0.1218],\n",
            "          [ 0.2062,  0.1960,  0.1858,  ..., -0.1708, -0.1751, -0.1794]]]],\n",
            "       grad_fn=<UpsampleBilinear2DBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading Task"
      ],
      "metadata": {
        "id": "YEvd8pm16TSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/dataset'\n",
        "\n",
        "#image_dir = \"/content/drive/MyDrive/dataset/COVERAGE/train/images\"\n",
        "#image_names = os.listdir(image_dir)\n",
        "\n",
        "# Path to the COVERAGE folder\n",
        "coverage_path = os.path.join(dataset_path, 'COVERAGE')\n",
        "\n",
        "# Path to the train and valid folders\n",
        "train_path = os.path.join(coverage_path, 'train')\n",
        "valid_path = os.path.join(coverage_path, 'valid')\n",
        "\n",
        "# Function to load and preprocess the images\n",
        "#import torch\n",
        "\n",
        "def load_images(path):\n",
        "    image_list = []\n",
        "    for file in os.listdir(path):\n",
        "        if file.endswith('.jpg'):\n",
        "            image_path = os.path.join(path, file)\n",
        "            image = cv2.imread(image_path)\n",
        "            image = cv2.resize(image, (256, 256))  # Reshape image to 256x256\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image_list.append(image)\n",
        "    return np.array(image_list)\n",
        "\n",
        "\n",
        "# Load and preprocess the training images\n",
        "train_images_path = os.path.join(train_path, 'images')\n",
        "train_images = load_images(train_images_path)\n",
        "train_images = torch.from_numpy(train_images).permute(0, 3, 1, 2).float() / 255.0  # Convert to tensor, adjust shape and normalize\n",
        "\n",
        "# Load and preprocess the validation images\n",
        "valid_images_path = os.path.join(valid_path, 'images')\n",
        "valid_images = load_images(valid_images_path)\n",
        "valid_images = torch.from_numpy(valid_images).permute(0, 3, 1, 2).float() / 255.0  # Convert to tensor, adjust shape and normalize\n",
        "\n",
        "train_masks_path = os.path.join(train_path, 'masks')\n",
        "train_masks = load_images(train_masks_path)\n",
        "train_masks = torch.from_numpy(train_masks).permute(0, 3, 1, 2).float() / 255.0  # Convert to tensor, adjust shape and normalize\n",
        "\n",
        "# Load and preprocess the validation mask images\n",
        "valid_masks_path = os.path.join(valid_path, 'masks')\n",
        "valid_masks = load_images(valid_masks_path)\n",
        "valid_masks = torch.from_numpy(valid_masks).permute(0, 3, 1, 2).float() / 255.0  # Convert to tensor, adjust shape and normalize\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Calculate the mean values of training images and masks\n",
        "train_images_mean = torch.mean(train_images, dim=(0, 2, 3))\n",
        "train_masks_mean = torch.mean(train_masks, dim=(0, 2, 3))\n",
        "\n",
        "# Calculate the mean values of validation images and masks\n",
        "valid_images_mean = torch.mean(valid_images, dim=(0, 2, 3))\n",
        "valid_masks_mean = torch.mean(valid_masks, dim=(0, 2, 3))\n",
        "\n",
        "# Define the transformations for training images and masks\n",
        "#train_transform = transforms.Compose([\n",
        " #   transforms.ToPILImage(),  # Convert the tensor image to a PIL image\n",
        "  #  transforms.Resize((256, 256)),  # Resize image to a fixed size (e.g., 256x256)\n",
        "   # transforms.ToTensor(),\n",
        "    #transforms.Normalize(train_images_mean, [1.0, 1.0, 1.0]),  # Use the mean values of training images\n",
        "#])\n",
        "\n",
        "\n",
        "\n",
        "#train_mask_transform = transforms.Compose([\n",
        " #   transforms.ToPILImage(),\n",
        " #   transforms.Resize((256, 256)),\n",
        " #   transforms.ToTensor(),\n",
        " #   transforms.Normalize(train_masks_mean, [1.0]),  # Use the mean value of training masks (gray scale)\n",
        "#])\n",
        "\n",
        "# Define the transformations for validation images and masks\n",
        "#val_transform = transforms.Compose([\n",
        " #   transforms.ToPILImage(),\n",
        " #   transforms.Resize((256, 256)),\n",
        " #   transforms.ToTensor(),\n",
        " #   transforms.Normalize(valid_images_mean, [1.0, 1.0, 1.0]),  # Use the mean values of validation images\n",
        "#])\n",
        "\n",
        "#val_mask_transform = transforms.Compose([\n",
        " #   transforms.ToPILImage(),\n",
        " #   transforms.Resize((256, 256)),\n",
        " #   transforms.ToTensor(),\n",
        " #   transforms.Normalize(valid_masks_mean, [1.0]),  # Use the mean value of validation masks (gray scale)\n",
        "#])*/"
      ],
      "metadata": {
        "id": "4-yycsf16WYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import pdb\n",
        "\n",
        "\n",
        "class COVERAGE(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform =None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        #self.mask_transform = mask_transform\n",
        "        self.images = os.listdir(image_dir)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        #img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.images[index])\n",
        "\n",
        "        #print(os.listdir(mask_path))\n",
        "        #pdb.set_trace()\n",
        "\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        mask  = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
        "        mask[mask == 255.0] = 1.0\n",
        "\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=image, mask=mask)\n",
        "            image = augmentations[\"image\"]\n",
        "            mask = augmentations[\"mask\"]\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "71t8GavW6wXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_loaders(\n",
        "    train_dir,\n",
        "    train_maskdir,\n",
        "    val_dir,\n",
        "    val_maskdir,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "):\n",
        "\n",
        "    train_ds = COVERAGE(\n",
        "       image_dir=train_dir,\n",
        "       mask_dir=train_maskdir,\n",
        "       transform=train_transform,\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "       train_ds,\n",
        "       batch_size=batch_size,\n",
        "       num_workers=num_workers,\n",
        "       pin_memory=pin_memory,\n",
        "       shuffle=True,\n",
        "    )\n",
        "\n",
        "    val_ds = COVERAGE(\n",
        "       image_dir=val_dir,\n",
        "       mask_dir=val_maskdir,\n",
        "       transform=val_transform,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n"
      ],
      "metadata": {
        "id": "lIuJbOmH69up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_accuracy(loader, model, device=\"cuda\"):\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0.0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device).unsqueeze(1)\n",
        "            p1, p2 = model(x)\n",
        "            preds = torch.sigmoid(p2)\n",
        "            preds = (preds > 0.5).float()\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_pixels += torch.numel(preds)\n",
        "            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n",
        "    print(\n",
        "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f}\"\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Dice score: {dice_score/len(loader)}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "l0T_Ja7YeuNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 20\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_HEIGHT = 256 #1280\n",
        "IMAGE_WIDTH = 256  #1918\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = True\n",
        "TRAIN_IMG_DIR = \"/content/drive/MyDrive/dataset/COVERAGE/train/images/\"\n",
        "TRAIN_MASK_DIR = \"/content/drive/MyDrive/dataset/COVERAGE/train/masks/\"\n",
        "VAL_IMG_DIR = \"/content/drive/MyDrive/dataset/COVERAGE/valid/images/\"\n",
        "VAL_MASK_DIR = \"/content/drive/MyDrive/dataset/COVERAGE/valid/masks/\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0.0\n",
        "    all_predictions = []\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(loader):\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.unsqueeze(1).to(device=DEVICE)\n",
        "        print(\"Target type:\", type(targets))\n",
        "\n",
        "        # Print the statement and the shape of data\n",
        "        #print(f\"Shape of data: {data.shape}\")\n",
        "        # Print the statement and the shape of targets\n",
        "        #print(f\"Shape of targets: {targets.shape}\")\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions, pred = model(data)\n",
        "            print(\"prediction type:\", type(predictions))\n",
        "\n",
        "            print(predictions.shape)\n",
        "            print(pred.shape)\n",
        "\n",
        "            #all_predictions.append(predictions)\n",
        "            #print(\"Shape of all predictions:\", all_predictions.shape)\n",
        "            print(\"Shape of targets:\", targets.shape)\n",
        "            loss = loss_fn(pred, targets)\n",
        "\n",
        "\n",
        "\n",
        "        #pdb.set_trace()\n",
        "        #all_predictions = tuple(torch.cat(tensors, dim=0) for tensors in zip(*all_predictions))\n",
        "        #targets = torch.cat(targets, dim=0)\n",
        "        '''\n",
        "        for i, prediction in enumerate(all_predictions):\n",
        "          loss = loss_fn(prediction, targets[i])\n",
        "        '''\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Accumulate loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print progress\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"Batch {batch_idx}/{len(loader.dataset)} Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Calculate average loss\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    print(f\"Epoch Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qqfEMpBq7N5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from albumentations import Compose, Normalize, Resize, Rotate, HorizontalFlip, VerticalFlip\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    train_transform = A.Compose(\n",
        "        [\n",
        "           A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "           A.Rotate(limit=35, p=1.0),\n",
        "           A.HorizontalFlip(p=0.5),\n",
        "           A.VerticalFlip(p=0.1),\n",
        "\n",
        "           A.Normalize(\n",
        "               mean=[0.0, 0.0, 0.0],\n",
        "               std=[1.0, 1.0, 1.0],\n",
        "               max_pixel_value=255.0,\n",
        "           ),\n",
        "           ToTensorV2(),\n",
        "        ],\n",
        "        is_check_shapes=False\n",
        "    )\n",
        "\n",
        "\n",
        "    val_transforms = A.Compose(\n",
        "        [\n",
        "           A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "           A.Normalize(\n",
        "               mean=[0.0, 0.0, 0.0],\n",
        "               std=[1.0, 1.0, 1.0],\n",
        "               max_pixel_value=255.0,\n",
        "           ),\n",
        "           ToTensorV2(),\n",
        "        ],\n",
        "        is_check_shapes=False\n",
        "    )\n",
        "\n",
        "    model = get_mvss(backbone='resnet50', pretrained_base=True, nclass=1, sobel=True, n_input=3, constrain=True)\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    train_loader, val_loader = get_loaders(\n",
        "        TRAIN_IMG_DIR,\n",
        "        TRAIN_MASK_DIR,\n",
        "        VAL_IMG_DIR,\n",
        "        VAL_MASK_DIR,\n",
        "        BATCH_SIZE,\n",
        "        train_transform,\n",
        "        val_transforms,\n",
        "        NUM_WORKERS,\n",
        "        PIN_MEMORY,\n",
        "    )\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
        "\n",
        "    check_accuracy(val_loader, model, device=DEVICE)\n",
        "\n"
      ],
      "metadata": {
        "id": "DV3jTnz77b5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr6XMd8E7mQz",
        "outputId": "ef92e4ab-0cdc-4106-c4f0-522f02409eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load pretrain success\n",
            "----------use sobel-------------\n",
            "----------use constrain-------------\n",
            "load pretrain success\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target type: <class 'torch.Tensor'>\n",
            "actual shape check: torch.Size([16, 3, 256, 256])\n",
            "device of x: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1, 64, 64])\n",
            "torch.Size([16, 1, 256, 256])\n",
            "prediction type: <class 'torch.Tensor'>\n",
            "torch.Size([16, 1, 64, 64])\n",
            "torch.Size([16, 1, 256, 256])\n",
            "Shape of targets: torch.Size([16, 1, 256, 256])\n",
            "Batch 0/80 Loss: 1.3742\n",
            "Target type: <class 'torch.Tensor'>\n",
            "actual shape check: torch.Size([16, 3, 256, 256])\n",
            "device of x: cpu\n",
            "torch.Size([16, 1, 64, 64])\n",
            "torch.Size([16, 1, 256, 256])\n",
            "prediction type: <class 'torch.Tensor'>\n",
            "torch.Size([16, 1, 64, 64])\n",
            "torch.Size([16, 1, 256, 256])\n",
            "Shape of targets: torch.Size([16, 1, 256, 256])\n",
            "Target type: <class 'torch.Tensor'>\n",
            "actual shape check: torch.Size([16, 3, 256, 256])\n",
            "device of x: cpu\n",
            "torch.Size([16, 1, 64, 64])\n",
            "torch.Size([16, 1, 256, 256])\n",
            "prediction type: <class 'torch.Tensor'>\n",
            "torch.Size([16, 1, 64, 64])\n",
            "torch.Size([16, 1, 256, 256])\n",
            "Shape of targets: torch.Size([16, 1, 256, 256])\n",
            "Target type: <class 'torch.Tensor'>\n",
            "actual shape check: torch.Size([16, 3, 256, 256])\n",
            "device of x: cpu\n",
            "torch.Size([16, 1, 64, 64])\n",
            "torch.Size([16, 1, 256, 256])\n",
            "prediction type: <class 'torch.Tensor'>\n",
            "torch.Size([16, 1, 64, 64])\n",
            "torch.Size([16, 1, 256, 256])\n",
            "Shape of targets: torch.Size([16, 1, 256, 256])\n",
            "Target type: <class 'torch.Tensor'>\n",
            "actual shape check: torch.Size([16, 3, 256, 256])\n",
            "device of x: cpu\n",
            "torch.Size([16, 1, 64, 64])\n",
            "torch.Size([16, 1, 256, 256])\n",
            "prediction type: <class 'torch.Tensor'>\n",
            "torch.Size([16, 1, 64, 64])\n",
            "torch.Size([16, 1, 256, 256])\n",
            "Shape of targets: torch.Size([16, 1, 256, 256])\n",
            "Epoch Loss: -25.4079\n",
            "Target type: <class 'torch.Tensor'>\n",
            "actual shape check: torch.Size([16, 3, 256, 256])\n",
            "device of x: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hXTWPvP54e0B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}